# -*- coding: utf-8 -*-
"""Adisa_Interview Exercise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iOVw6PfkjW9o7HQpUWH9cWOMV9pO1ekV

# **Use the PDBe API to retrieve information about the co-crystallized ligands of SOS1

Son of Sevenless homolog 1**

Retrieve PDB Entries for SOS1 and mount drive
"""

from google.colab import drive
drive.mount('/content/drive')

job_description = '/content/drive/MyDrive/InterviewExercise'
import os
files = os.listdir(job_description)
print(files)

"""# Task 1
**Use the PDBe API to retrieve information about the co-crystallized ligands of SOS1. Leverage the JSON format to produce a well-organized CSV (index=PDBID) containing the most valuable data. Three-letter code and SMILES reppresentation are compulsory features.**
"""

import requests # import the request module

uniprot_id = 'Q07889' # SOS1 UniprotID
url = f"https://www.ebi.ac.uk/pdbe/api/mappings/best_structures/Q07889" # to get the PDB entries that contain the compound from PDBe API url
# this retrieves the best PDB structure mappings available for SOS1 Q07889

entries = requests.get(url) # the request module help get data from the url

import json
#To be sure if the requests get was succesful
if entries.status_code == 200:
  print("Request successful")
  SOS1_data = entries.json()
  print(SOS1_data)
else:
  SOS1_data = None
  print(f"Data retrieval failed with status code: {entries.status_code}")
  print(entries.text)

import pandas as pd
import numpy as np
SOS1_json = pd.read_json('/content/drive/MyDrive/InterviewExercise/SOS1.json')
SOS1_json.head()
# save to csv
SOS1_csv = '/content/drive/MyDrive/InterviewExercise/SOS1.csv'
SOS1_json.to_csv(SOS1_csv, index='pdb_id')
SOS1_csv_df = pd.read_csv(SOS1_csv)
SOS1_json.head()

"""The above dataframe does not look proper!!"""

import json # import json

with open ('/content/drive/MyDrive/InterviewExercise/SOS1.json') as jf:
  data = json.load(jf)

data.keys()

# data.values() # this code prints all the values in Q07889

df = pd.json_normalize(data, record_path="Q07889") # this normalises the list of dictionaries to a proper csv DataFrame
df

df['pdb_id'] # PDB Entries associated with SOS1 Q07889

!pip install rdkit-pypi # instal rdkit

"""**Extract the co-crystalized Ligands**"""

# Step 1: Get PDB IDs associated with the protein (using UniProt ID)
uniprot_id = 'Q07889'  # Example: SOS1 protein
url = f"https://www.ebi.ac.uk/pdbe/api/mappings/best_structures/{uniprot_id}"

try:
    response = requests.get(url)
    response.raise_for_status()  # this checks for HTTP errors
    data = response.json()

    # Extract PDB IDs
    pdb_ids = [entry['pdb_id'] for entry in data.get(uniprot_id, [])]

    if not pdb_ids:
        print("No PDB IDs found for this UniProt ID.")
    else:
        # Step 2: For each PDB ID, retrieve ligand information including SMILES
        for pdb_id in pdb_ids:
            ligand_url = f'https://www.ebi.ac.uk/pdbe/api/pdb/entry/ligand_monomers/{pdb_id}'

            ligands_response = requests.get(ligand_url)

            if ligands_response.status_code == 200:
                ligands_data = ligands_response.json()

                if pdb_id in ligands_data:
                    print(f"PDB ID: {pdb_id}")
                    for ligand in ligands_data[pdb_id]:
                        print(f"Ligand ID (HET code): {ligand['chem_comp_id']}")
                        print(f"Ligand Name: {ligand['chem_comp_name']}")

                        # Print the entire ligand dictionary to understand its structure
                        print("Ligand Dictionary:", ligand)
                        # Print the keys of the dictionary to see if 'smiles' is present
                        print("Keys in Ligand Dictionary:", ligand.keys())
                        # Check if 'smiles' key is in the ligand dictionary
                        if 'smiles' in ligand:
                          smiles = ligand['smiles']
                          if smiles:
                            print(f"SMILES: {smiles}")
                          else:
                            print("SMILES information is present but empty.")
                        else:
                          print("SMILES information is not available in the ligand.")

                        dataset = print()
                else:
                    print(f"No ligands found for PDB ID: {pdb_id}")
            else:
                print(f"Error retrieving ligands for PDB ID: {pdb_id} - Status Code: {ligands_response.status_code}")
            print("-" * 40)
except requests.exceptions.RequestException as e:
    print(f"An error occurred: {e}")

import json
import requests
import urllib
from urllib.request import urlopen
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole

# Step 1: Get PDB IDs associated with the protein (using UniProt ID)
uniprot_id = 'Q07889'
url = f"https://www.ebi.ac.uk/pdbe/api/mappings/best_structures/{uniprot_id}"

all_ligand_data = []  # Initialize a list to store all ligand data

try:
    response = requests.get(url)
    response.raise_for_status()
    data = response.json()

    # Extract PDB IDs
    pdb_ids = [entry['pdb_id'] for entry in data.get(uniprot_id, [])]

    if not pdb_ids:
        print("No PDB IDs found for this UniProt ID.")
    else:
        # Step 2: For each PDB ID, retrieve ligand information including SMILES
        for pdb_id in pdb_ids:
            ligand_url = f'https://www.ebi.ac.uk/pdbe/api/pdb/entry/ligand_monomers/{pdb_id}'

            ligands_response = requests.get(ligand_url)

            if ligands_response.status_code == 200:
                ligands_data = ligands_response.json()

                if pdb_id in ligands_data:
                    for ligand in ligands_data[pdb_id]:
                        ligand_info = {
                            'pdb_id': pdb_id,
                            'ligand_id': ligand['chem_comp_id'],
                            'ligand_name': ligand['chem_comp_name'],
                            'smiles': ligand.get('smiles', '')  # Extract smiles
                        }
                        all_ligand_data.append(ligand_info)
                else:
                    print(f"No ligands found for PDB ID: {pdb_id}")
            else:
                print(f"Error retrieving ligands for PDB ID: {pdb_id} - Status Code: {ligands_response.status_code}")
            print("-" * 40)

except requests.exceptions.RequestException as e:
    print(f"An error occurred: {e}")

# present in dataframe
df = pd.DataFrame(all_ligand_data)

# Save the DataFrame to a JSON file
output_json_file = 'ligand_data.json'
df.to_json(output_json_file, orient='records')

print(f"Ligand data saved to: {output_json_file}")

"""**Did not verbose, but its printed already, I can call this or put in DataFrame for visualization**"""

all_ligand_data

df2 = pd.DataFrame(all_ligand_data)
df2

"""**here is it**"""

df2.set_index('pdb_id', inplace=True)
df2 = df.reset_index(drop=True)
df2

df2 = df2.rename(columns={'pdb_id': 'PDBID', 'ligand_id': 'CODE', 'ligand_name': 'LIGAND_NAME'})
df2

all_ligand_data = '/content/drive/MyDrive/InterviewExercise/all_ligand_data.csv' # save to drive directory

"""**To get the list of the SMILES of the assoiated Ligands, we need the pubchempy module**"""

!pip install pubchempy # install pubchempy

ligands = df2['LIGAND_NAME']
ligands # view the co-crystalized ligands

"""**Get the SMILES here**"""

import pubchempy as pcp
def get_smiles(ligands):
    try:
        compound = pcp.get_compounds(ligands, 'name')
        if compound:
            return compound[0].canonical_smiles
        else:
            return None
    except Exception as e:
        return None

# Apply the function to each row in the DataFrame
df2['SMILES'] = df2['LIGAND_NAME'].apply(get_smiles)
df2

"""# **Task 2
** From the previusly made CSV retrieve the IC50/Kd values from ChemBL/BindingDB to determine if any of the co-crystallized ligands have tabulated IC50.**

**Trying with either BindingDB or ChemBL**
"""

# Try querying BindingDB
ligands = df2['LIGAND_NAME']
def get_bindingdb_data(compound_name, activity_type="IC50"):
  for ligand in ligands:

    url = f"https://bindingdb.org/bind/BindingDBrestful/getLigandsByIC50/{ligand}"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.text
        if data:
            return pd.read_csv(pd.compat.StringIO(data))
        else:
            return None
    else:
        return None

# Apply the function to each row in the DataFrame
bindingdb_data = get_bindingdb_data(ligands)

# Display the data
if bindingdb_data is not None and not bindingdb_data.empty:
    print(bindingdb_data[['Ki (nM)', 'Kd (nM)', 'IC50 (nM)', 'PubChem CID']])
else:
    print("No data found.")

"""**Using BindingDB shows that IC50/Kd values are not available** *No Data Found*

Try ChemBL
"""

# let's try  ChemBL, but we need to install the extra client for this
!pip install chembl_webresource_client # install chembl web resource client to access the chembl ids of the ligands

# now get the chemBL ids to query the chemBL API
from chembl_webresource_client.new_client import new_client
def get_chembl_id(ligands):
    url = f"https://www.ebi.ac.uk/chembl/api/data/molecule.json?pref_name={ligands}&limit=1"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        if data['molecules']:
            return data['molecules'][0]['molecule_chembl_id']
    return None

ligands = df2['LIGAND_NAME']
#df = pd.DataFrame(ligands)

# Apply the function to each ligand name to get ChEMBL IDs
df2['ChEMBL_ID'] = ligands.apply(get_chembl_id)

df2 # some ligands have chembl id available while some returned none

df2.isna().sum()

"""**There are 322 ligands with unavailable ChemBL_IDs**

But lets see the available ones
"""

chembl_ids = df2.dropna(subset=['ChEMBL_ID'])
chembl_ids

"""**There are 211 rows with chembl_ids, so we can do this by iterating through the list**"""

import requests
import pandas as pd

# Function to fetch bioactivity data for a ChEMBL ID from ChEMBL API
def get_bioactivity_data(chembl_ids):
    url = f"https://www.ebi.ac.uk/chembl/api/data/activity.json?molecule_chembl_id={chembl_ids}"
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        activities = data.get('activities', [])

        # Collect relevant data (IC50, Ki, Kd)
        bioactivity_data = []
        for activity in activities:
            activity_type = activity.get('standard_type')
            standard_value = activity.get('standard_value')
            standard_units = activity.get('standard_units')
            if activity_type in ['IC50', 'Ki', 'Kd']:
              for chembl_id in chembl_ids:
                bioactivity_data.append({
                    'chembl_id': chembl_id,
                    'activity_type': activity_type,
                    'value': standard_value,
                    'units': standard_units
                })
        return bioactivity_data
    else:
        print(f"Error fetching data for ChEMBL ID: {chembl_ids} - Status Code: {response.status_code}")
        return []


chembl_ids = chembl_ids.to_numpy()
chembl_ids = chembl_ids.flatten()
chembl_ids = chembl_ids.tolist()

# Initialize a list to collect all bioactivity data
all_bioactivity_data = []

# Loop through the ChEMBL IDs and collect bioactivity data
for chembl_id in chembl_ids:
    bioactivity_data = get_bioactivity_data(chembl_id)
    all_bioactivity_data.extend(bioactivity_data)

# Convert the list of bioactivity data to a DataFrame
bioactivity_df = pd.DataFrame(all_bioactivity_data)

# Display the resulting bioactivity DataFrame
if not bioactivity_df.empty:
    print(bioactivity_df)
else:
    print("No bioactivity data found for the given ChEMBL IDs.")

"""**Wow!!!!, Finally, The query of chemBL API  with chembl_ids succesfully returned the data, for available activity/affinity types IC50/k/ki data hence next task!**

*Lets visualize this in DataFrame*
"""

# Convert the list of bioactivity data to a DataFrame
bioactivity_df = pd.DataFrame(all_bioactivity_data)
bioactivity_df

unique_activity_types = bioactivity_df['activity_type'].unique()
print(unique_activity_types)

"""**The unique function shows that only IC50 and ki are available**

# **Task 3
**From ChemBL/BindingDB the activities of the ligands screened against the target.**

**To retrieve activities of ligands found in SOS1 from chemBL**

**We need to get the target_chembl_id**

**Let try this programmatically, else, we do manualy**
"""

# To retrieve activities of ligands found in SOS1 from chemBL
# We need to get the target_chembl_id
#Let try this programmatically, else, we do manualy.

uniprot_id = 'Q07889'
def get_target_chembl_id(uniprot_id):
    url = f"https://www.ebi.ac.uk/chembl/api/data/target.json?target_components__accession={uniprot_id}"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        if data['targets']:
            return data['targets'][0]['target_chembl_id']
    return None


target_chembl_id = get_target_chembl_id(uniprot_id)

print(f"Target ChEMBL ID for SOS1 (Q07899): {target_chembl_id}")

"""**Beautiful, chembl_id fo SOS1 Q07889 is CHEMBL2079846**

***Retrieve Ligand Activities!!***
"""

SOS1_ID = 'CHEMBL2079846'
def get_chembl_activities(SOS1_ID, activity_type="IC50"):
    url = f"https://www.ebi.ac.uk/chembl/api/data/activity.json?target_chembl_id={target_chembl_id}&activity_type={activity_type}&limit=1000"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return data['activities']
    else:
        return None

# Apply the function to get activities
activity_types = ["IC50", "kd", "ki"]   # You can specify other types like "Kd", "Ki", etc.
for activity_type in activity_types:
  activities = get_chembl_activities(SOS1_ID, activity_type)

# Convert to a DataFrame for easier analysis
if activities:
    df_activity = pd.DataFrame(activities)
    print(df_activity[['molecule_chembl_id', 'value', 'units', 'assay_description']])
else:
    print("No activity data found.")

df_activity.head()

"""**This is the table of activities of the co-crystalized ligands associated with SOS1 with uniprot_id Q07899**"""

df_activity.info() #get full info

df_activity.describe()

"""**The activity types of the ligands available**"""

df_activity['type']

"""# **Build a QSAR model**

QSAR = Quantitative Structure Activity Relationship.

This model will estimate how the structures of the ligands will predict the activity/affinity of the respective ligands.

These avtivities could be:

IC50: Half maximal inhibitors concentration

ki: Inhibition constant

kd: dissociation constant

**To do these,** we need to convert chemical structures to molecular descriptors.

**the descriptors needed for this model are:

**the molecular weight and Logp**

**molecular weight can be gotten from their smiles programatically**

**then LogP from the molecular weights**
"""

# I want to use RandomForestRegressor estimator/model.
from rdkit import Chem
from rdkit.Chem import Descriptors # we need to convert chemical structures to molecular descriptors

# the descriptors needed for this model are the molecular weight and Logp
# molecular weight can be gotten from their smiles
# then LogP from the molecular weights

# read smiles from the DataFrame df2 with information about co-crystalized ligands
canonical_smiles = df2['SMILES']
molecules = df2['LIGAND_NAME']

# Calculate molecular descriptors, i.e molecular weight and LogP
def safe_mol_from_smiles(smiles):
    try:
        return Chem.MolFromSmiles(smiles)
    except:
        return None

df2['molecule'] = df2['SMILES'].apply(lambda x: safe_mol_from_smiles(x))
molecular_weight = []
LogP = []
for mol in df2['molecule']:
  if mol is not None:
    molecular_weight.append(Descriptors.MolWt(mol))
    LogP.append(Descriptors.MolLogP(mol))
    print(f"Molecular Weight: {molecular_weight}")
    print(f"LogP: {LogP}")
  else:
    molecular_weight.append(None)
    LogP.append(None)

# turn them to DataFrame to visualize better
df3 = pd.DataFrame(molecular_weight)
df4 = pd.DataFrame(LogP)

df3 # molecular weight

df4 #LogP

"""**Merge molecular weight and Logp to a single DataFrame**"""

# merge molecular weight and LogP in a DataFrame
merge = pd.concat([df3, df4], axis=1)

merge.columns = ['molecular_weight', 'LogP']
merge

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the distribution of molecular weights
plt.figure(figsize=(7, 5))
sns.histplot(merge['molecular_weight'], bins=30, kde=True)
plt.title('Distribution of Molecular Weights')
plt.xlabel('Molecular Weight')
plt.ylabel('Frequency')

"""**Show relationship/correlation type between molecular weight and Logp**"""

plt.plot(merge['molecular_weight'], merge['LogP'], 'D')
plt.xlabel('Molecular Weight')
plt.ylabel('LogP')
plt.title('Molecular Weight vs. LogP')
plt.show()

"""**It is rougly a positive correlation**
**Logp increases with molecular weight**

**This portrays a positive correlation and a Linear Regression estimator can be used to model the system**

**Incorporate molecular weight and Logp into df2**
"""

# add them to df2
df2['molecular_weight'] = df3
df2['LogP'] = df4
df2

unique_ligand = df2['LIGAND_NAME'].unique()
unique_ligand

len(unique_ligand), len(df2['LIGAND_NAME'])

df_activity['standard_value'] # standard vaule is an important activity feature of the ligands

df2.info()

df_activity.info()

"""**Check if the number of rows of the molecular weight/Logp are consistent**"""

X = df2[['molecular_weight', 'LogP']]
y = df_activity['standard_value']
print(len(X))
print(len(y))

"""**Building model on this X and y split will return error of inconsistence dimensions, hence needs to be trimmed to match as follows**"""

if len(X) > len(y):
    X_trimmed = X.iloc[:len(y)]
    y_trimmed = y
elif len(y) > len(X):
    y_trimmed = y.iloc[:len(X)]
    X_trimmed = X
else:
    X_trimmed = X
    y_trimmed = y

print(len(X_trimmed))
print(len(y_trimmed)) # Now the dimensions are equal and can be modeled with Regression model (RandonForestRegressor)

"""**The number of rows are equal from sampling**
**can be modeled with Regression model (RandonForestRegressor)**
"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import explained_variance_score
from sklearn.metrics import median_absolute_error

#  convert standard values to float data type and drop NAN rows
y = df_activity['standard_value'].astype(float).dropna()
X = df2[['molecular_weight', 'LogP']].dropna()  # Drop rows with missing descriptors

if len(X) > len(y):
    X_trimmed = X.iloc[:len(y)]
    y_trimmed = y
elif len(y) > len(X):
    y_trimmed = y.iloc[:len(X)]
    X_trimmed = X
else:
    X_trimmed = X
    y_trimmed = y

# Split the data into training and testing sets
X_trimmed_train, X_trimmed_test, y_trimmed_train, y_trimmed_test = train_test_split(X_trimmed, y_trimmed,
                                                    test_size=0.2,
                                                    random_state=42)
# random_state does the work of random seed to retrict to a set of the possible random combinations

# Create a Random Forest Regressor model
model = RandomForestRegressor() # instantiate the estimator

# Fit the model to the training data
model.fit(X_trimmed_train, y_trimmed_train)

# Make predictions on the test data
y_pred = model.predict(X_trimmed_test)

# Evaluate the model
mse = mean_squared_error(y_trimmed_test, y_pred)
r2 = r2_score(y_trimmed_test, y_pred)
mae = mean_absolute_error(y_trimmed_test, y_pred)
rmse = np.sqrt(mse)
evs = explained_variance_score(y_trimmed_test, y_pred)
median_ae = median_absolute_error(y_trimmed_test, y_pred)


print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
print(f"Mean Absolute Error: {mae}")
print(f"Root Mean Squared Error: {rmse}")
print(f"Explained Variance Score: {evs}")
print(f"Median Absolute Error: {median_ae}")

!python -m pip install https://github.com/MolecularAI/QSARtuna/releases/download/3.1.1/qsartuna-3.1.1.tar.gz

"""**Try this modelling with QSARtuna**"""

!python --version

"""**My python version mismatches the requirement for QSARtuna, hence not installed, So I stick to RandomForest Regressor model**"""

from google.colab import output
output.enable_custom_widget_manager()

"""Support for third party widgets will remain active for the duration of the session. To disable support:

# **Task 3 Perform Molecular Matched Pair Analysis**
**I am using rdkit**

Molecular Matched Pair Analysis (MMPA) is a method used in medicinal chemistry and cheminformatics to analyze the impact of small structural changes (transformations) on the properties of chemical compounds. The goal is to understand how specific molecular modifications affect biological activity, physicochemical properties, or other relevant metrics, such as solubility, toxicity, or pharmacokinetics.
"""

from rdkit import Chem # rDkit = Reaction Data kit
from rdkit.Chem import rdFMCS # rdFMCS = RDkit Find Maximum Common Substructure

# df2' contains my molecules/ligands with SMILES
smiles_list = df2['SMILES'].dropna().tolist()  # Read as list from DataFrame, removing missing SMILES

# Perform MMP analysis
pairs = []
for i in range(len(smiles_list)):
    for j in range(i + 1, len(smiles_list)):
        mol1 = Chem.MolFromSmiles(smiles_list[i])
        mol2 = Chem.MolFromSmiles(smiles_list[j])
        if mol1 and mol2:
            mcs = rdFMCS.FindMCS([mol1, mol2])
            if mcs.numAtoms > 0:  # Check if a common substructure was found
                pairs.append((smiles_list[i], smiles_list[j], mcs.smartsString))
# SMARTS = SMiles ARbitrary Target Specification (designed for querying and Identifying patterns or substructures within molecule)
# SMILES = Simplified Molecular Input Line Entry (Reperesents whole molecule)
# Result processing and Analysis
print("MMP Analysis Results:")
for smiles_a, smiles_b, mcs_smarts in pairs:
    print(f"Molecule 1: {smiles_a}")
    print(f"Molecule 2: {smiles_b}")
    print(f"MCS SMARTS: {mcs_smarts}")
    print("-" * 40)
# present this in dataframe for easy visualizzation
#df_match_pair

"""**DataFrame of matched pairs**"""

# present this in dataframe for easy visualizzation
df_match_pair = pd.DataFrame(pairs, columns=['Molecule 1', 'Molecule 2', 'MCS SMARTS'])
df_match_pair.head(10)

"""# **Perfom Free Wilson Analysis**

Free-Wilson analysis is a method used in medicinal chemistry and quantitative structure-activity relationship (QSAR) studies to assess how different chemical substituents influence biological activity. It involves systematically modifying the substituents on a core molecule and determining how these modifications affect the activity of the compound.
"""

!pip install mordred
from mordred import Calculator, descriptors

!pip install papyrus-python

"""**papyrus version not installing, Hence I am using the model method for Free-wilson analysis**

Estimator = LinearRegressor form scikit learn linear model
"""

from scipy.stats import linregress
from sklearn.linear_model import LinearRegression

import pandas as pd

# standard_value from the activity data is set as target for our model.
y = df_activity['standard_value'].astype(float).dropna() # standard value reperesent the biological activity of the molecule
X = df2[['molecular_weight', 'LogP']].dropna()  # Drop rows with missing descriptors

# One-hot encode the presence/absence of each substructure (from MMP analysis)
substructures = list(set([mcs for _, _, mcs in pairs]))
substructure_presence = pd.DataFrame(index=df2.index, columns=substructures)
for i, row in df2.iterrows():
    for substructure in substructures:
        substructure_presence.loc[i, substructure] = int(substructure in row[['SMILES']]) # gets the presence of the substructure as true or false 'boolean' and returned it as int

# Combine descriptors and substructure presence
X = pd.concat([X, substructure_presence], axis=1).dropna()

# Align dataframes to have the same indices
common_index = X.index.intersection(y.index)
X = X.loc[common_index]
y = y.loc[common_index]

# Perform linear regression (Free-Wilson analysis)
model = LinearRegression()
model.fit(X, y)

# Analyze coefficients
coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})
print(coefficients.sort_values('Coefficient', key=abs, ascending=False))

# present this in dataframe for easy visualization
coefficients.sort_values('Coefficient', key=abs, ascending=False)

"""Analysis of the Coefficients:
LogP (-581.896214):

**Interpretation:** LogP has a strong negative coefficient, indicating that as LogP increases (molecules become more hydrophobic), the biological activity decreases significantly.
Possible Reason: A high LogP suggests that a molecule is more hydrophobic, which might reduce its solubility or increase its likelihood of being trapped in cell membranes, thus negatively impacting its biological activity in certain assays.

**Molecular Weight (7.460326):**

**Interpretation:** Molecular weight has a positive coefficient, suggesting that as the molecular weight increases, biological activity increases (though the effect is much smaller than LogP's impact).
Possible Reason: Heavier molecules might interact more strongly with their targets or possess more functional groups, enhancing biological activity. However, this positive effect is relatively minor.

**Substructures with Zero Coefficients (0.000000):**

[#6]-,:#6-,:[...] (Feature 457)

[#6]-,:[#6](:[#6]:#6-[#6]-[#7]:,-[#6]... (Feature 458)

**Interpretation:** These substructures have zero coefficients, indicating that the presence or absence of these specific substructures does not contribute to biological activity in this dataset.

Possible Reason: The substructures might not be relevant to the activity being studied or may not vary significantly among the molecules, thus having no discernible impact on the outcome.

# **Docking**

**Needed tools:

Gnina,

Smina,

rdkit,

pandas,

maptplotlib

**

**From documentation, Gnina is not a python package, but can be run on python script with subprocees**
"""

import requests
import os

# Define the PDB ID and output file path
pdb_id = "5OVF"
output_file = "5OVF.pdb"

# Construct the URL to download the PDB file from rcsb pdb site
url = f"https://files.rcsb.org/download/{pdb_id}.pdb"

# Download the PDB file
response = requests.get(url)

# Save the PDB file
with open(output_file, "wb") as f:
    f.write(response.content)

print(f"PDB file {pdb_id}.pdb downloaded successfully.")

"""**lets view the structure**"""

!pip install pymol-open-source

"""**pymol not installing**

**Lets try nglview**
"""

#lets try nglview
!pip install nglview

import nglview as nv

# Load the PDB file
view = nv.show_file("5OVF.pdb")

# Display the structure
view

view.render_image()

"""Support for third party widgets will remain active for the duration of the session. To disable support:"""

from google.colab import output
output.enable_custom_widget_manager()

"""Support for third party widgets will remain active for the duration of the session. To disable support:

**Not showing properly with NGL vieing lets try py3dmol**
"""

!pip install py3Dmol # install Py3Dmol

import py3Dmol

# Create a viewer
view = py3Dmol.view(width=400, height=300)

# Load the PDB file content
with open('5OVF.pdb', 'r') as file:
    pdb_data = file.read()

# Add the structure to the viewer
view.addModel(pdb_data, 'pdb')

# Set the visualization style (e.g., cartoon, sticks)
view.setStyle({'cartoon': {'color': 'spectrum'}})

# Center and zoom
view.zoomTo()

# Show the structure

view.show()

"""**Beautiful, structure is visualized on Py3Dmol**"""

from google.colab import output
output.enable_custom_widget_manager()

"""**The pdb file format is not compactible for docking on docking tools, we need to convert to pdbqt**

**Install openbabel, to convert pdb to pdbqt**
"""

# install openbabel to help convert the pdb file for the docking executable pdbqt format
!apt-get update
!apt-get install -y openbabel

"""# Docking Task 1
**write an automated pipeline using Smina to perform a virtual screening experiment on SOS1 (PDB ID: 5OVF) with at least 10 small molecules that are both biologically relevant and synthetically accessible.**
"""

# Commented out IPython magic to ensure Python compatibility.
# Install dependencies
!apt-get install -y cmake libboost-all-dev

# Clone the Gnina repository
!git clone https://github.com/gnina/gnina.git

# Navigate to the gnina directory
# %cd gnina

# Build Gnina
!mkdir build
# %cd build
!cmake ..
!make

# Install Gnina
!make install

"""# Installing Gnina gave a lot of problem, I want to try Smnia"""

!apt-get update
!apt-get install -y build-essential cmake wget openbabel

# Build smina from source
!apt-get update
!apt-get install -y wget

# clone smina
!git clone https://github.com/mwojcikowski/smina.git

"""**Docking with Smina**"""

# Prepare protein
!obabel 5OVF.pdb -O protein.pdbqt -xr -p 7.4

# Download sample biologically relevant and synthetically accessible molecules
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/ace.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/benzene.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/ind.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/pyridine.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/toluene.sdf
# Add more molecules here (at least 10) ...

# Convert ligands to pdbqt
!obabel *.sdf -O ligands.pdbqt -m

# Define docking parameters
receptor = "protein.pdbqt"
ligands = "ligands.pdbqt"
out = "docking_results.sdf"
center_x = -7.5
center_y = 23.5
center_z = 21.0
size_x = 20
size_y = 20
size_z = 20

# Run Smina
!smina --receptor $receptor --ligand $ligands --center_x $center_x --center_y $center_y --center_z $center_z --size_x $size_x --size_y $size_y --size_z $size_z --out $out

!ls -l smina

!apt-get install -y openbabel

!pip install openbabel-wheel

!apt-get update
!apt-get install -y build-essential cmake

# Commented out IPython magic to ensure Python compatibility.
# Install dependencies
!apt-get install -y build-essential cmake wget openbabel

# Clone the Smina repository (if not already cloned)
!git clone https://github.com/mwojcikowski/smina.git

# Navigate to the smina directory
# %cd smina

# Build Smina (if not already built)
!mkdir build
# %cd build
!cmake .. # CMake is a build system generator that automates the process of configuring and generating build files for software projects across different platforms.
!make

# Install Smina
!make install
!make

# Go back to the main directory
# %cd ../..

# Prepare protein
!obabel 5OVF.pdb -O protein.pdbqt -xr -p 7.4

# Download sample biologically relevant and synthetically accessible molecules  5ovf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/ace.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/benzene.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/ind.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/pyridine.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/toluene.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/adamantane.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/benzocaine.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/butane.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/caffeine.sdf
!wget -q https://raw.githubusercontent.com/mwojcikowski/smina/master/test/ethanol.sdf

# Convert ligands to pdbqt
!obabel *.sdf -O ligands.pdbqt -m

# Define docking parameters
receptor = "protein.pdbqt"
ligands = "ligands.pdbqt"
out = "docking_results.sdf"
center_x = -7.5
center_y = 23.5
center_z = 21.0
size_x = 20
size_y = 20
size_z = 20

# Run Smina
!smina --receptor $receptor --ligand $ligands --center_x $center_x --center_y $center_y --center_z $center_z --size_x $size_x --size_y $size_y --size_z $size_z --out $out

"""**Smina giving issues, even after succefully installed and cloned, I want to try plants**"""

# confirming if openbabel is successfully installed
import openbabel
from openbabel import pybel

print("OpenBabel and Pybel are successfully installed!")

"""**I cant get through with Gnina, Smina and Plants, I need further education, But i can make do with autodock vina**

**Trying with autodock vina**
"""

# Install required packages
!apt-get update
!apt-get install -y python3-pip
!pip install biopython

# Install AutoDock Vina
!wget https://github.com/ccsb-scripps/AutoDock-Vina/releases/download/v1.2.3/vina_1.2.3_linux_x86_64.tar.gz
!tar -xzf vina_1.2.3_linux_x86_64.tar.gz
!chmod +x vina

from openbabel import pybel
import subprocess

ligands_sdf = '/content/drive/MyDrive/Colab Notebooks/ligands.sdf'
receptor_pdb = '/content/drive/MyDrive/Colab Notebooks/5OVF.pdb'

# Define paths
receptor_pdb = '5OVF.pdb'
ligands_sdf = 'ligands.sdf'
receptor_pdbqt = 'receptor.pdbqt'
ligands_pdbqt = 'ligands.pdbqt'

# Convert receptor PDB to PDBQT format
receptor = pybel.readfile('pdb', receptor_pdb).__next__() # Since pybel.readfile() returns an iterator, this command gets the first (and only) receptor molecule in the file.
receptor.removeh()  # Remove hydrogens
receptor.addh()     # Add hydrogens
receptor.write('pdbqt', receptor_pdbqt, overwrite=True)

# Convert ligands SDF to PDBQT format
ligands = pybel.readfile('sdf', ligands_sdf)
for ligand in ligands:
    ligand.addh()  # Add hydrogens
    ligand.write('pdbqt', ligands_pdbqt, overwrite=True)

print(f"Converted receptor to {receptor_pdbqt} and ligands to {ligands_pdbqt}.")

# Create AutoDock Vina configuration file
config_file = 'config.txt'
with open(config_file, 'w') as f:
    f.write(f"""# AutoDock Vina configuration file
    receptor = {receptor_pdbqt}
    ligand = {ligands_pdbqt}
    center_x = 0
    center_y = 0
    center_z = 0
    size_x = 20
    size_y = 20
    size_z = 20
    num_modes = 9
    exhaustiveness = 8
    """)
import subprocess

# Define the output file path
output_file = 'docking_output.pdbqt'
log_file = 'docking_log.txt'

# Run AutoDock Vina
result = subprocess.run(['./vina', '--config', config_file, '--out', output_file, '--log', log_file], capture_output=True, text=True)

# Print results
print('Vina Output:')
print(result.stdout)
print('Vina Errors:')
print(result.stderr)

# Print docking log
with open(log_file, 'r') as f:
    log_contents = f.read()
print('Docking Log:')
print(log_contents)



"""# Docking Task 2
**Write an automated pipeline using Gnina (or alternative tools) to perform covalent docking on G12C. Utilize CovPDB to identify the appropriate warhead for covalent docking. Refer to the ChemBL/BindingDB ligands for inspiration.**
"""

!pip install covpdb
import covpdb
protein_id = '6OIM'
warheads = covpdb.get_warheads(protein_id)
print(warheads)

# Use cvpdb to identify warhead
!pip install covpdb
import covpdb
import requests
protein_id = '6OIM'


def fetch_covpdb_ligands(protein_id):
    url = f'https://covpdb.cc/ligands/{protein_id}'  # Assuming CovPDB API or web interface
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()  # if it returns ajson data
    else:
        print(f"Error fetching data from CovPDB: {response.status_code}")
        return None

def identify_warhead(ligands):
    for ligand in ligands:
        if ligand['type'] == 'warhead':  # Assuming the data categorizes the ligands
            return ligand['smiles']  # Return the SMILES of the warhead
    return None

# Step to execute for 6OIM
protein_id = '6OIM'  # PDB ID for the target protein
ligands = fetch_covpdb_ligands(protein_id)

if ligands:
    warhead_smiles = identify_warhead(ligands)
    if warhead_smiles:
        print(f"Identified warhead for covalent docking in 6OIM: {warhead_smiles}")
    else:
        print("No warhead identified in CovPDB data for 6OIM.")
else:
    print("No ligand data retrieved from CovPDB.")

"""# Docking Task 2

**write a complete pipeline that:**

**Enumeratesthe GBB Reaction using suitably selected building blocks (from Reaxys or Enamine) obtained via SMARTS strings**
"""

# Hint:
# smarts_string = "[#7:3]~[#6:1]-[#7:2].[C-:4]#[N+:5].[#6:7]=O>>[#7:5]-[#6:4]-1=[#6:7]-[#7:2]=[#6:1]-[#7:3]-1"

!pip install rdkit-pypi
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole

# Define the GBB reaction SMARTS string
smarts_string = "[#7:3]~[#6:1]-[#7:2].[C-:4]#[N+:5].[#6:7]=O>>[#7:5]-[#6:4]-1=[#6:7]-[#7:2]=[#6:1]-[#7:3]-1"

# Create a reaction from the SMARTS string
rxn = AllChem.ReactionFromSmarts(smarts_string)


# Building blocks
building_blocks = [
    ("[#7H]~C-N", "amine"),  # Primary amine
    ("C#N", "isonitrile"),  # Isonitrile
    ("C=O", "aldehyde"),  # Aldehyde
]

# Generate possible products
products = []
for amine_smarts, amine_name in building_blocks:
  for isonitrile_smarts, isonitrile_name in building_blocks:
    for aldehyde_smarts, aldehyde_name in building_blocks:
      amine = Chem.MolFromSmarts(amine_smarts)
      isonitrile = Chem.MolFromSmarts(isonitrile_smarts)
      aldehyde = Chem.MolFromSmarts(aldehyde_smarts)
      if amine and isonitrile and aldehyde:
        try:
          ps = rxn.RunReactants((amine, isonitrile, aldehyde))
          for p in ps:
            products.append((p[0], f"{amine_name}-{isonitrile_name}-{aldehyde_name}"))
        except:
          pass

# Draw the products
if products:
  for product, name in products:
    print(name)
    Draw.MolToImage(product)
else:
  print("No products found.")



"""# **Molecular Dynamics Simulations of 5OVF Protein with GROMACS**
** Task 1**
**GROMACS: GROningen MAchine for Chemical Simulations**
"""

# call the parameter files from drive
from google.colab import drive
drive.mount('/content/drive')

# confirm availability of my parameter files on drive
files = os.listdir('/content/drive/MyDrive/Colab Notebooks')
print(files)
import pandas as pd
fl =pd.DataFrame(files)
fl.to_csv('files.csv' , index=False)
fl

# call the parameter files from drive
ions = '/content/drive/MyDrive/Colab Notebooks/ions.mdp'
minim = '/content/drive/MyDrive/Colab Notebooks/minim.mdp'
nvt = '/content/drive/MyDrive/Colab Notebooks/nvt.mdp'
npt = '/content/drive/MyDrive/Colab Notebooks/npt.mdp'
md = '/content/drive/MyDrive/Colab Notebooks/md.mdp'

with open(ions, 'r') as file:
    content_ions = file.read()
with open(minim, 'r') as file:
    content_minim = file.read()
with open(nvt, 'r') as file:
  content_nvt = file.read()
with open(npt, 'r') as file:
  content_npt = file.read()
with open(md, 'r') as file:
  content_md = file.read
content_ions
content_minim
content_nvt
content_npt
content_md

"""# Simple Molecular Dynamics Simulation of 5OVF with Gromacs
**GROMACS:**
Gronigen Machine for Molecular Simulation
"""

# Commented out IPython magic to ensure Python compatibility.

# Create a directory for the simulation
!mkdir simulation

# Copy the protein file to the simulation directory
!cp 5OVF.pdb simulation/

# Go to the simulation directory
# %cd simulation

# Generate topology
!gmx pdb2gmx -f 5OVF.pdb -o 5OVF_processed.gro -ff amber99sb -water tip3p

# Define box
!gmx editconf -f 5OVF_processed.gro -o 5OVF_boxed.gro -c -d 1.0 -bt cubic

# Solvate the system
!gmx solvate -cp 5OVF_boxed.gro -cs spc216.gro -o 5OVF_solvated.gro -p topol.top

# Add ions
!gmx grompp -f /content/drive/MyDrive/Colab\ Notebooks/ions.mdp -c 5OVF_solvated.gro -p topol.top -o ions.tpr
!gmx genion -s ions.tpr -o 5OVF_ionized.gro -p topol.top -pname NA -nname CL -neutral

# Energy minimization
!gmx grompp -f /content/drive/MyDrive/Colab\ Notebooks/minim.mdp -c 5OVF_ionized.gro -p topol.top -o em.tpr
!gmx mdrun -v -deffnm em -cpi

# NVT equilibration
!gmx grompp -f /content/drive/MyDrive/Colab\ Notebooks/nvt.mdp -c em.gro -p topol.top -o nvt.tpr
!gmx mdrun -v -deffnm nvt -cpi

# NPT equilibration
!gmx grompp -f /content/drive/MyDrive/Colab\ Notebooks/npt.mdp -c nvt.gro -p topol.top -o npt.tpr
!gmx mdrun -v -deffnm npt -cpi

# Production MD run
!gmx grompp -f /content/drive/MyDrive/Colab\ Notebooks/md.mdp -c npt.gro -p topol.top -o md.tpr
!gmx mdrun -v -deffnm md -cpi

# Analyze the trajectory (example: RMSD)
!gmx rms -s md.tpr -f md.xtc -o rmsd.xvg -tu ns

# Go back to the main directory
# %cd ..

# Convert the PDB file to Gromacs format
!obabel 5OVF.pdb -O 5OVF.gro

from google.colab import output
output.enable_custom_widget_manager()

"""Support for third party widgets will remain active for the duration of the session. To disable support:

# **Exploration**
**Reinvent4**
Generation and optimization of novel chemical structures that could potentially be developed into drugs.
"""

# Assume we have a dataset of known SOS1 inhibitors in SMILES format
sos1_inhibitors = [
    "CC(C)Nc1ncnc2c1ncn2C1CCOC1",
    "CC(C)c1ccc(NC(=O)C2=Nc3ccc(N)c(Oc4ccc(Cl)cc4)c3N=C2)cc1",
    "O=C(Nc1ccc2c(c1)cc(n2)NC(=O)CCl)Nc1ccc2c(c1)cc(n2)NC(=O)CCl",
    "CC(C)C1=NC2=C(N1)N=C(NC2=O)C1CCCCC1",
    "CC1=C2N=C(NC3=C(N=CN3C)C2=NC=N1)C1CCCCC1"
]

# I am using just a simple representation instaed of the APIs and other tools
from rdkit import Chem
from rdkit.Chem import Draw

# Convert SMILES to RDKit molecules
rdkit_structures = [Chem.MolFromSmiles(smiles) for smiles in sos1_inhibitors]

# Visualization
structure_image = Draw.MolsToGridImage(molecules, molsPerRow=3)
structure_image

# Initialize REINVENT or a similar model and fine-tune on the SOS1 inhibitors dataset
# This step involves setting up the model, loading the dataset, and defining the reward function

# Generate new molecules
# This would typically involve running the model in inference mode and generating SMILES strings.
generated_smiles = [
    "CC(C)Nc1ccc(C2=NN=C(S2)N(C)C)cc1",  # smile 1
    "CC(C)C1=NN=C(N1)C1=NC=CC=N1C",       # smile 2
    "O=C(NC1=CC=CC=C1)N1CCCCN1C(=O)C",    # smile 3
    "CC1=NC(C)=C(C=C1)C1=CC=CN=C1",       # smile 4
    "C1CCN(CC1)C1=CC=CC=C1"               # smile 5
]

# Convert generated SMILES to RDKit molecules for visualization
generated_molecules = [Chem.MolFromSmiles(smiles) for smiles in generated_smiles]

# Visualize the generated molecules
img_generated = Draw.MolsToGridImage(generated_molecules, molsPerRow=3)
img_generated

